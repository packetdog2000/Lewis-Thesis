{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd,os,sys\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import colorConverter\n",
    "import gmplot\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "from scipy.spatial.distance import cdist,pdist\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'/Users/edwingarcia/Dropbox/_Lewis-University/CPSC-59000-Data Science Project/github/Lewis-Thesis/notebook'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FILES , DATAFRAMES, MODELS AND PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_FOLDER = \"../data/output\"\n",
    "SOURCE_FOLDER = \"../data/src\"\n",
    "\n",
    "## CREATE FOLDER IF OUTPUT FOLDER IS UNDEFINED\n",
    "if not os.path.exists(OUTPUT_FOLDER):\n",
    "    os.makedirs(OUTPUT_FOLDER)\n",
    "\n",
    "## IF NO 'data' FOLDER EXISTS, CRATE ONE...\n",
    "if not os.path.exists(SOURCE_FOLDER):\n",
    "    os.makedirs(SOURCE_FOLDER)\n",
    " \n",
    "## july-oct-raw2-train.csv\n",
    "# csv_file = \"july-oct-batch2-train.csv\" ## raw_input(\"Enter csv file to load:\")\n",
    "# csv_file = \"batch2-train.csv\" ## raw_input(\"Enter csv file to load:\")\n",
    "csv_file = \"july-oct-train.csv\" ## raw_input(\"Enter csv file to load:\")\n",
    "## USED TO CONVERT BACK FROM DECOMPOSED VARIABLES TO GEOCORDS\n",
    "raw_file = \"%s/july-oct-raw.csv\" %(SOURCE_FOLDER)\n",
    "raw_map_df = pd.read_csv(raw_file) \n",
    "\n",
    "file_str = \"%s/%s\" %(SOURCE_FOLDER, csv_file)\n",
    "a = pd.read_csv(file_str) \n",
    "a.columns = ['index1', 'address','city', 'day','hour','type','latitude','longitude','parent_incident','state']\n",
    "a.drop('index1',axis=1,inplace=True)\n",
    "\n",
    "X = a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'address', u'city', u'day', u'hour', u'type', u'latitude',\n",
       "       u'longitude', u'parent_incident', u'state'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RUN DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated number of clusters: 114\n",
      "Adjusted Rand Index: 1.000\n"
     ]
    }
   ],
   "source": [
    "## REDUCE DATA USING PCA\n",
    "pca = PCA(n_components=2).fit(X)\n",
    "pca_2d = pd.DataFrame(pca.transform(X))\n",
    "\n",
    "# Add PCA cols to DF\n",
    "X.loc[:,('pca1')] = pca_2d[0]\n",
    "X.loc[:,('pca2')] = pca_2d[1]\n",
    "\n",
    "min_samples_list = np.arange(10,11,2)\n",
    "epsilon = 0.037 ## 0.01905\n",
    "plt.figure()\n",
    "for i in min_samples_list:\n",
    "    db = DBSCAN(eps=epsilon, min_samples=i).fit(pca_2d)\n",
    "    db_pred = db.fit_predict(pca_2d)\n",
    "    core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "    core_samples_mask[db.core_sample_indices_] = True\n",
    "    labels = db.labels_\n",
    "\n",
    "    # Number of clusters in labels, ignoring noise if present.\n",
    "    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    total_outliers = len(pca_2d) - len(db.core_sample_indices_)\n",
    "    # Black removed and is used for noise instead.\n",
    "    unique_labels = set(labels)\n",
    "    fig,(ax1,ax2) = plt.subplots(1,2,figsize=(16,6))\n",
    "    colors = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels)))\n",
    "    for k, col in zip(unique_labels, colors):\n",
    "        if k == -1:\n",
    "            # Black used for noise.\n",
    "            col = 'k'\n",
    "\n",
    "        class_member_mask = (labels == k)\n",
    "\n",
    "        \n",
    "        xy = X[class_member_mask & ~core_samples_mask]\n",
    "        outliers = ax1.scatter(xy['pca1'], xy['pca2'], marker='^', c='c', s=15, alpha=0.3) ## centroids points\n",
    "        inliers = ax1.scatter(xy['pca1'], xy['pca2'], marker='s', c='m',s=3,alpha=0.4) \n",
    "\n",
    "#         outliers = ax1.plot(xy['pca1'], xy['pca2'], '.', markerfacecolor='m', mec='r', markersize=5, alpha=0.8) ## centroids points\n",
    "#         inliers = ax1.plot(xy['pca1'], xy['pca2'], '.', markerfacecolor='b', mec='w', markersize=3,alpha=0.6) \n",
    "        \n",
    "        xy = X[class_member_mask & core_samples_mask]\n",
    "        clusters1 = ax1.scatter(xy['pca1'], xy['pca2'], c=col, marker='o', s=15, alpha=0.5)\n",
    "        clusters2 = ax2.scatter(xy['pca1'], xy['pca2'], c=col,marker='o', s=10, alpha=0.7)\n",
    "        \n",
    "        title1 =\"DBSCAN With Outliers and Core Points\\nEpsilon=%s, Min. Samples=%s,Clusters=%s, Outliers=%s\" %(\n",
    "            epsilon, i, n_clusters_,total_outliers)\n",
    "        title2 =\"DBSCAN With Clusters Only\\nEpsilon=%s, Min. Samples=%s, Clusters=%s\" %(epsilon, i, n_clusters_)\n",
    "        ax1.legend([outliers,inliers],['Outliers','Core Points'])\n",
    "        ax2.legend([clusters2],['Clusters'])\n",
    "        ax1.set_title(title1)\n",
    "        ax2.set_title(title2)\n",
    "        ax1.set_ylabel(\"PCA 1\")\n",
    "        ax2.set_ylabel(\"PCA 1\")\n",
    "        ax1.set_xlabel(\"PCA 2\")\n",
    "        ax2.set_xlabel(\"PCA 2\")\n",
    "        \n",
    "    print('Estimated number of clusters: %d' % n_clusters_)\n",
    "    print(\"Adjusted Rand Index: %0.3f\" % metrics.adjusted_rand_score(labels, db_pred))\n",
    "#     print(\"Silhouette Coefficient: %0.3f\" % metrics.silhouette_score(pca_2d, db_pred))\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### REVERT BACK TO ORIGINAL COORDINATES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epsilon = 0.037 # 0.01905 # \n",
    "min_samples = 10\n",
    "db = DBSCAN(eps=epsilon, min_samples=min_samples).fit(pca_2d)\n",
    "db_pred = db.fit_predict(pca_2d)\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "labels = db.labels_\n",
    "clust_centers = db.core_sample_indices_\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GET CENTROIDS AND MATCH TO RAW DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a_labeled = a\n",
    "a_labeled['label'] = labels\n",
    "a_centroids = pd.DataFrame([])\n",
    "gmap_centroids = pd.DataFrame([])\n",
    "for i in clust_centers:\n",
    "    a_centroids = a_centroids.append(a_labeled.ix[i])\n",
    "    gmap_centroids = gmap_centroids.append(raw_map_df.ix[i])\n",
    "\n",
    "## Match the labels for the gmap coords\n",
    "gmap_centroids['label'] = a_centroids['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(a['longitude'],a['latitude'],marker='.', c='k',s=5,alpha=0.4)\n",
    "plt.scatter(a_centroids['longitude'],a_centroids['latitude'],c=a_centroids['label'],s=20, marker='o',edgecolors='w', alpha=0.7)\n",
    "\n",
    "title_str = \"DBSCAN With Centroids Mapped into Raw Dataset\\nEpsilon=%s, Min. Samples=%s, Clusters=%s\" %(epsilon,min_samples,\n",
    "n_clusters_)\n",
    "plt.title(title_str)\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GENERATE GOOGLE MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## GENERATE GOOGLE MAP IN SEPARATE HTML FILE\n",
    "\n",
    "gmap = gmplot.GoogleMapPlotter(29.8, -95.4, 9.0)\n",
    "gmap.scatter(raw_map_df['latitude'], raw_map_df['longitude'], '#3B0B39', alpha=0.4, size=60, marker=False)\n",
    "gmap.heatmap(gmap_centroids['latitude'], gmap_centroids['longitude'],radius=(20))\n",
    "\n",
    "## PATH IS OUTPUT FOLDER/FILENAME + SEQUENCE NUMBER.html\n",
    "map_file = \"%s/%s-EPS-%s-MIN-SAMPLES-%s.html\" %(OUTPUT_FOLDER,\"DBSCAN\",epsilon, min_samples)\n",
    "gmap.draw(map_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GENERATE KML/KMZ FILE FOR MAP LAYER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter name of .kml file you wish to create: test1\n"
     ]
    }
   ],
   "source": [
    "kml_name = raw_input(\"Enter name of .kml file you wish to create: \")\n",
    "kml = gmap_centroids[['latitude','longitude','label']]\n",
    "kml_output_str = \"%s/dbscan-%s-%s.kml.csv\" %(OUTPUT_FOLDER,epsilon, kml_name)\n",
    "kml.to_csv(kml_output_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### END OF  DBSCAN CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### REFERENCES:\n",
    "Part of the code source comes from  scikitlearn website:\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html#sklearn.cluster.DBSCAN"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
