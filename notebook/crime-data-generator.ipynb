{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LEWIS UNIVERSITY \n",
    "#### DATA SCIENCE PROJECT\n",
    "##### DATASET: HARRIS COUNTY CRIME STATISTICS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Capstone in Computer Science: Detecting Patterns in Criminal Activities Using K-means and DBSCAN Clustering\n",
    "    - Edwin Garcia\n",
    "    - MS Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys, os,math,time\n",
    "from datetime import datetime\n",
    "import pandas as pd, numpy as np\n",
    "import getpass\n",
    "import urllib\n",
    "import geocoder\n",
    "from geopy.geocoders import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Set min and max coordinates for Harris County Texas USA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "min_max_lat = (30.2, 29.2) ## Min and Max latitude for filtering outlier coords\n",
    "min_max_long = (-96.1,-94.8) ## Min and Max longitude for filtering outlier coords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  - METHOD: get_categories(param1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_categories(cat1):\n",
    "    '''\n",
    "    GET PROPERTY THEFT RELATED CRIME CATEGORIES, param1 = dataframe\n",
    "    '''\n",
    "    ## GET PROPERTY THEFT RELATED CRIME CATEGORIES\n",
    "    cat1 = raw.groupby('incident_type_primary').sum()\n",
    "    cat2 = cat1.reset_index()\n",
    "    cat3 = cat2[['incident_type_primary']]\n",
    "    cat4 = pd.DataFrame(cat3[cat3['incident_type_primary'].str.contains('Burglar|Robber|Theft|Stole')]['incident_type_primary'])\n",
    "    return cat4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - METHODS - GEOCODER FUNCTION GET ZIP CODE AND CITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_zip(lat_, long_):\n",
    "    '''\n",
    "    Get the ZIP/Postal Code, latitude and longitude parameters.\n",
    "    Returns 5-digit postal code\n",
    "    '''\n",
    "    g = geocoder.google([lat_,long_], method='reverse')\n",
    "    return g.postal\n",
    "\n",
    "def get_street_block(lat_, long_):\n",
    "    '''\n",
    "    Get the address block, latitude and longitude parameters.\n",
    "    Returns a string address number and street, i.e., 1200 Elm street\n",
    "    '''\n",
    "    g = geocoder.google([lat_,long_], method='reverse')\n",
    "    block = int(round(float(g.housenumber)/1000,1) * 1000)\n",
    "    ret_str = \"%s Block %s\" %(block, g.street)\n",
    "    return ret_str\n",
    "\n",
    "def get_city(lat_, long_):\n",
    "    '''\n",
    "    Get the city latitude and longitude parameters.\n",
    "    Returns string city name\n",
    "    '''\n",
    "    g = geocoder.google([lat_,long_], method='reverse')\n",
    "    return g.city\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### METHOD: clean_data(param1) - CLEANS THE DATA FOR ANALYSIS - param1=dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    '''\n",
    "    dataset cleaner function\n",
    "    this removes missing data, incomplete locations,etc.\n",
    "    also removes non-essential columns\n",
    "    '''\n",
    "    df['location'].dropna(axis=0,inplace=True)\n",
    "        \n",
    "    # remove non-essentail cols\n",
    "    df.drop('clearance_type',axis=1,inplace=True)\n",
    "    df.drop('updated_at',axis=1,inplace=True)\n",
    "    df.drop('case_number',axis=1,inplace=True)\n",
    "    df.drop('incident_id',axis=1,inplace=True)\n",
    "    # raw.drop('country',axis=1,inplace=True)\n",
    "    \n",
    "    ## remove items without incident datetime     \n",
    "    print \"checking incident date time info...\"\n",
    "    df.drop([i for i,j in df.iterrows() if pd.isnull(j.incident_datetime)], inplace=True)\n",
    "    print \"\\nincident date time info check completed...\\n\"\n",
    "    \n",
    "    # fix null locations\n",
    "    print \"checking null locations ...\"\n",
    "    df.loc[[i for i,j in df.iterrows() if pd.isnull(j.location)], 'location']= get_street_block(j.latitude, j.longitude)\n",
    "\n",
    "    print \"\\nnull location check completed...\\n\"\n",
    "    \n",
    "    # remove latitude beyond harris (gt 30.3 and lt 29.3)\n",
    "    print \"checking invalid latitude values ...\"\n",
    "    df.drop([i for i,j in df.iterrows() if float(j.latitude) > min_max_lat[0] or float(j.latitude) < min_max_lat[1]], inplace=True)\n",
    "    print \"\\ninvalid latitude values check completed...\\n\"\n",
    "    \n",
    "    # remove longitude beyond harris (gt -96.1 and lt -94.8)\n",
    "    print \"checking invalid longitude values ...\"\n",
    "    df.drop([i for i,j in df.iterrows() if float(j.longitude) < min_max_long[0] or float(j.longitude) > min_max_long[1]], inplace=True)\n",
    "    print \"\\ninvalid longitude values check completed...\\n\"\n",
    "    \n",
    "    ## some zip codes are in zip column, move them in state col\n",
    "    ## if nan, set to 0\n",
    "    print \"checking state === TX...\"\n",
    "    df.loc[[i for i,j in df.iterrows() if(j.state =='TX' and not pd.isnull(j.zip))], 'state']= get_zip(j.latitude, j.longitude)\n",
    "    df.loc[[i for i,j in df.iterrows() if(j.state =='null' or pd.isnull(j.state))], 'state']=get_zip(j.latitude, j.longitude)\n",
    "    print \"\\nstate==TX check completed...\\n\"\n",
    "    \n",
    "    ## fix row if state is still TX\n",
    "    print \"checking if both state and zip === 'null'(str) ...\"\n",
    "    df.loc[[i for i,j in df.iterrows() if(j.state =='TX')], 'state']= get_zip(j.latitude, j.longitude)\n",
    "   \n",
    "    ## fix row if state is still null\n",
    "    print \"checking if both state and zip === 'null'(str) ...\"\n",
    "    df.loc[[i for i,j in df.iterrows() if(j.state =='' and pd.isnull(j.zip))], 'state']= get_zip(j.latitude, j.longitude)\n",
    "    print \"\\nstate and zip null string check completed...\\n\"\n",
    "    \n",
    "    ## FILL IN MISSING DAYS OF WEEK\n",
    "    print \"checking if days of week are missing ...\"\n",
    "    days={0:'Monday', 1:'Tuesday', 2:'Wednesday', 3:'Thursday', 4:'Friday', 5:'Saturday', 6:'Sunday'}\n",
    "    df.loc[[i for i,j in df.iterrows() for k in days \n",
    "            if(j.day_of_week =='null' or pd.isnull(j.day_of_week))], 'day_of_week'] = days[pd.to_datetime(j.incident_datetime).dayofweek] \n",
    "    print \"\\nmissing day check completed...\\n\"\n",
    "        \n",
    "    # REMOVE EXCESS INDEXES\n",
    "    df.drop('zip', axis=1,inplace=True)\n",
    "    df.drop(df.columns[0:0],axis=1,inplace=True)\n",
    "    df.reset_index(inplace=True)\n",
    "    \n",
    "    print \"\\nfiltering for property crimes...\\n\"  \n",
    "    ## GET CATEGORIES \n",
    "    categories = get_categories(raw) # get unique categories\n",
    "    df = pd.DataFrame([j for i,j in df.iterrows() for k,l in categories.iterrows() if j['incident_type_primary'] in l['incident_type_primary']])\n",
    "    print \"\\ncategory filtering completed...\\n\"\n",
    "    \n",
    "    print \"Data Cleaning Completed!\"\n",
    "        \n",
    "    # RETURN DATAFRAME\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive Input and Variable generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_SOURCE_LOCATION = raw_input(\"Load dataset from local file or from API?\\nEnter 1 = local file, 2 = API:\")\n",
    "DATA_FOLDER =\"data\"\n",
    "OUTPUT_FOLDER= \"output\"\n",
    "\n",
    "## CREATE FOLDER IF OUTPUT FOLDER IS UNDEFINED\n",
    "if not os.path.exists(OUTPUT_FOLDER):\n",
    "    os.makedirs(OUTPUT_FOLDER)\n",
    "\n",
    "## IF NO 'data' FOLDER EXISTS, CRATE ONE...\n",
    "if not os.path.exists(DATA_FOLDER):\n",
    "    os.makedirs(DATA_FOLDER)\n",
    "        \n",
    "## IF LOADING DATA FROM LOCAL FILE\n",
    "if DATA_SOURCE_LOCATION == \"1\": \n",
    "    RAW_FILENAME = raw_input('enter the csv file to be loaded: ')\n",
    "    RAW_FILEPATH= \"%s/%s\" %(DATA_FOLDER, RAW_FILENAME)\n",
    "    raw = pd.read_csv(RAW_FILEPATH, encoding='utf-8')\n",
    "    \n",
    "    \n",
    "## IF LOADING DATA DIRECTLY FROM API FILE\n",
    "else:\n",
    "    URL = \"moto.data.socrata.com\"\n",
    "    DATASET = getpass.getpass(\"You are accessingdata set inhttps:socrata.com.\\nPlease enter the dataset name: \")\n",
    "    API_TOKEN= getpass.getpass('enter API KEY: ')\n",
    "    RAW_FILENAME = raw_input('enter a dataset name: ')\n",
    "    \n",
    "    \n",
    "    ## DATE RANGE FILTERS\n",
    "    START_DATE = raw_input(\"Enter start of date range (YYYY-MM-DD): \")\n",
    "    END_DATE = raw_input(\"Enter end of date range (YYYY-MM-DD): \")\n",
    "    start_str = \"created_at>'%sT00:00:00'\" %START_DATE\n",
    "    end_str = \"created_at<'%sT00:00:00'\" %END_DATE\n",
    "    \n",
    "    RAW_FILEPATH= \"%s/%s.csv\" %(DATA_FOLDER, RAW_FILENAME)\n",
    "    \n",
    "    ### API PARAMS\n",
    "#     SEARCH_FILTER = \"created_at > '2016-11-11T00:00:00' AND created_at < '2016-11-15T00:00:00'\"\n",
    "    SEARCH_FILTER = start_str+\"%20AND%20\"+end_str\n",
    "#     SEARCH_FILTER = \"created_at>'2016-09-01T00:00:00'%20AND%20created_at<'2016-11-10T00:00:00'\"\n",
    "    QUERY_LIMIT = 200000\n",
    "    QUERY_OFFSET = 0\n",
    "    \n",
    "    ## INSTANTIATE API FUNCTION USING URL AND API_KEY\n",
    "#     client = Socrata(URL, API_TOKEN)\n",
    "    ## CALL GET METHOS AND USE PARAMS\n",
    "#     search_str =\"created_at>'2016-09-01T00:00:00'%20AND%20created_at<'2016-11-10T00:00:00'\"\n",
    "    query_str =\"https://%s/resource/%s.json?$$app_token=%s&$where=%s&$limit=%s\" %(URL,DATASET,API_TOKEN,SEARCH_FILTER,QUERY_LIMIT)\n",
    "    query =(query_str)\n",
    "    raw = pd.read_json(query)\n",
    "\n",
    "\n",
    "    ## SAVE REUSLTS TO CSV\n",
    "    raw.to_csv(RAW_FILEPATH,index=False)\n",
    " \n",
    "\n",
    "## ASK FOR NAME OF OUTPUT FILE\n",
    "OUTPUT_FILENAME = raw_input('enter csv output file name: ')\n",
    "CLEANED_OUTPUTPATH = \"%s/%s\" %(OUTPUT_FOLDER,OUTPUT_FILENAME)\n",
    "print \"Proceed to next step...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_crime_types = len(raw['incident_type_primary'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run clean_data() function and assign to new DF. Then show statistics of DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a1 = clean_data(raw)\n",
    "a1.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export cleaned file to output (csv) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## EXPORT TO CSV\n",
    "a1.to_csv(CLEANED_OUTPUTPATH, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### END OF CODE"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
